<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>My Hakyll Blog - Reinforcement Learning in Haskell - Dynamic Programming</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">My Hakyll Blog</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <h1>Reinforcement Learning in Haskell - Dynamic Programming</h1>

            <div class="info">
    Posted on April 29, 2020
    
</div>

<h1 id="reinforcement-learning">Reinforcement Learning</h1>
<p><em>OK, I’ll bite. What’s Reinforcement Learning?</em><br />
Suppose you wanted to build a program to play a game like Space Invaders, or Chess, or Go. Any game where you pick up rewards along the way (in Space Invaders) or at the end (by winning in Chess), and you want to maximise your score. That’s what Reinforcement Learning is for.</p>
<p><em>So it’s just for games?</em><br />
It’s the field for any kind of automated goal seeking - like, for example, self-driving cars.</p>
<p><em>And Dynamic Programming?</em><br />
I’m getting there!</p>
<h1 id="approach-1-dynamic-programming">Approach #1: Dynamic Programming</h1>
<p><em>It sounds like the thing someone might shout in a Japanese cartoon. <em>DYNAMIC PROGRAMMING!</em></em></p>
<p>Dynamic Programming is breaking up your problems into subproblems, solving those, putting it all together. Like building a house one room at a time. I’ll talk through it with the code.</p>
<p>This is an extension to make our code a bit cleaner: &gt; {-# LANGUAGE RecordWildCards #-}</p>
<p>Name our module:<br />
&gt; module ReinforcementLearning.DP where</p>
<p>Don’t worry about these imports, I’ll explain them when they become relevant.<br />
&gt; import qualified Control.Foldl as L<br />
&gt; import Data.Map.Strict (Map)<br />
&gt; import qualified Data.Map.Strict as Map<br />
&gt; import qualified Data.Set as Set<br />
&gt; import Data.Maybe (maybe)</p>
<p>We’re gonna think about what we mean by a ‘game’ here. When you’re playing Space Invaders, you don’t need to remember how you’ve played up to this point; everything you need to know (your points, how healthy your barriers are, etc) are on the screen. In Chess, everything is represented by your board state.</p>
<p>This idea of ‘everything you need to know is on the board’ is called the <em>Markov Property</em>, and games which have this property and you need to make decisions at each point are called <em>Markov Decision Problems</em>.</p>
<p>Let’s define a representation of MDPs with states of type s and rewards of type r.<br />
&gt; data MDP r s = MDP<br />
We want to be able to get out a list of possible states: &gt; { states :: [s],<br />
We also want a list of things we can do if we’re in a state of type s &gt; actions :: s -&gt; [Action r s], And finally we’ll scale down future rewards by a ‘discount’ each time-step as punishment for taking too long. &gt; rewardDiscount :: Double &gt; }</p>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
